{
  "activation_function": "relu",
  "batch_size": 64,
  "epochs": 50,
  "hidden_layer_neurons": 50,
  "hidden_layers": 2,
  "inputs_columns": [24, 29, 4, 8, 9],
  "learning_rate": 0.01,
  "outputs": ["M", "B"],
  "outputs_column": "1",
  "plot_loss": true
}
